{"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","version":"3.7.7","file_extension":".py","pygments_lexer":"ipython3"},"notebookId":"19dc5bdb-fd61-4f30-88d9-f085908df1ef"},"cells":[{"cell_type":"markdown","source":"# CV – object detection\n\nВ этой тетрадке мы рассмотрим задачу детекции объектов на примере датасета [Stanford Drone Dataset](https://cvgl.stanford.edu/projects/uav_data/)\n\n**Предполагаем, что ноутбук запущен внутри Yandex DataSphere**","metadata":{"id":"cIu1b5Xo4Bdd","cellId":"38lqsetzmcbfb8lk82ubeg"}},{"cell_type":"code","source":"from pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport cv2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import Dataset, DataLoader, Sampler\nfrom torchvision.models import resnet34\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import transforms\n\nimport albumentations as A\nimport albumentations.pytorch.transforms as APT\nfrom albumentations.pytorch import ToTensorV2\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport datetime\nfrom PIL import Image\nfrom copy import deepcopy\nfrom collections import defaultdict\n\nimport os\n\nfrom skimage import io","metadata":{"id":"TyTa9uFb4Bdl","cellId":"212z2memf5cmplpkb5eb0s","trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Data\n\nStanford drone dataset – это датасет из нескольких видео, записанных с дрона в восьми местах (`SCENE_NAME`).\nКаждое видео покадрово размечено шестью типами объектов (`label`). (Подробное описание датасета лежит в файле `initial_README`)  \nВ текущей задаче мы будем использовать не исходные видео, а фреймы из них.\n\nСтруктура директории с датасетом:\n- ./stanford-drone-dataset-frames/\n    - {train, val}/\n        - annotations/\n            - {`SCENE_NAME`}/\n                - video{`VIDEO_ID`}/\n                    - annotations.csv\n    - {train, val}/\n        - frames/\n            - {`SCENE_NAME`}/\n                - video{`VIDEO_ID`}/\n                    - frame_{`FRAME_IDX`}.jpg\n\nДля каждого файла с аннотацией есть аналогичный путь к директории, в которой находятся фреймы из видео `frame_{FRAME_IDX}.jpg`.  \nНапример, для фреймов, лежащих в директории `./stanford-drone-dataset-frames/train/frames/quad/video0/`, соответствующий файл с разметкой лежит по пути: `./stanford-drone-dataset-frames/train/annotations/quad/video0/annotations.csv`.\n\nКаждая строка в файлах `annotations.csv` содержит аннотацию одного объекта. Каждый CSV файл содержит 11 колонок с хедером.\n\nНаиболее интересные нам колонки:\n- xmin – верхняя левая X-координата бокса объекта (bounding box).\n- ymin – верхняя левая Y-координата бокса объекта (bounding box).\n- xmax – нижняя правая X-координата бокса объекта (bounding box).\n- ymax – нижняя правая Y-координата бокса объекта (bounding box).\n- frame_idx – индекс фрейма, который соответсвует текущей строке аннотации.\n\nСэмпл из одной сцены можно скачать с [Google Drive](https://drive.google.com/file/d/18XeE0kHWqpLyBfZFbAbUTZqbygETTCLC/view?usp=sharing).","metadata":{"id":"CiLnuFXG4Bdm","cellId":"rgra31480u3h8nfb7hzj4"}},{"cell_type":"code","source":"# # Качаем архив с данными с Yandex Object Storage.\n\n# from cloud_ml.storage.api import Storage\n\n# s3 = Storage.s3(access_key=\"Le9tg70HQEJsoGqjqXH8\", secret_key=\"NV75mCPkC0PEd35ImyDI5vI7p40YGFOYZgkH7moa\")\n# # downloading contents of the remote file into the local one\n# s3.get_dir('dl-hse-2021/stanford-drone-dataset-frames/', './stanford-drone-dataset-frames/')","metadata":{"tags":[],"cellId":"bo51i7oqlpdb9m8dwn3spm","trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Задание 1\n**(0.2 балла)** Напишите класс датасет, который будет возвращать картинку и координаты размеченных на ней объектов.","metadata":{"id":"i8b_O3Y94Bdq","cellId":"zrykja70wsr3sw184uq2t7"}},{"cell_type":"markdown","source":"Обработка и преобразование датафреймов остались где-то за кадром. Если коротко говорить, то всю информацию объединил в 2 больших датафрейма. ","metadata":{"cellId":"30wz2lwoufd70zd57ly8eu"}},{"cell_type":"code","source":"class StanfordDroneDataset(Dataset):\n    def __init__(self, transform=None, csv='train.csv',\n                 folder_path='./stanford-drone-dataset-frames/'):\n        \n        self.df = self.get_df(csv)\n        self.folder_path = folder_path\n        self.transform = transform\n        \n    def get_df(self, csv):\n        df = pd.read_csv(csv)\n        df = df[df.lost != 1].reset_index().drop('index', axis = 1)\n        return df\n\n    def __len__(self):\n        return self.df.image_id.nunique()\n    \n    def __getitem__(self, index):\n        image_id = torch.tensor([index])\n        mini_df = self.df[self.df.image_id == index]\n        try:\n            image_path = mini_df.img_path.iloc[0]\n            num_objs = mini_df.shape[0]\n            # в датасете всего 2 лейбла\n            mini_df['Enc_labels'] = mini_df.label.apply(lambda x: 0 if x == 'Pedestrian' else 1)\n\n            image = io.imread(self.folder_path + image_path)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = torch.as_tensor(image, dtype=torch.int32)\n\n            #bboxes and labels\n            boxes = []\n            labels = []\n            for i in range(num_objs):\n                xmin = mini_df.xmin.iloc[i]\n                xmax = mini_df.xmax.iloc[i]\n                # тут какой-то косяк с bbox, подрезать будем так\n                while xmax / image.shape[1] > 1:\n                    xmax -= 1\n                ymin = mini_df.ymin.iloc[i]\n                ymax = mini_df.ymax.iloc[i]\n                while ymax / image.shape[0] > 1:\n                    ymax -= 1\n                label = mini_df.label.iloc[i]\n                boxes.append([xmin, ymin, xmax, ymax])\n                labels.append(mini_df['Enc_labels'].iloc[i])\n            boxes = torch.as_tensor(boxes, dtype=torch.int32)\n            labels = torch.as_tensor(labels, dtype=torch.int32)\n\n        \n            target = {}\n            target['image'] = image.numpy()\n            target[\"bboxes\"] = boxes.numpy()\n            target[\"labels\"] = labels.numpy()\n            \n        except:\n            target = {}\n            target['image'] = np.array([])\n            target[\"bboxes\"] = []\n            target[\"labels\"] = []\n        \n\n        if self.transform is not None:\n            target = self.transform(**target)\n\n        return target","metadata":{"cellId":"oc7biaz3ye06kf75gvciv5","trusted":true},"outputs":[],"execution_count":35},{"cell_type":"code","source":"CLASSES = ['Pedestrian', 'Biker']\ntrain_data = StanfordDroneDataset(csv='./stanford-drone-dataset-frames/train.csv')\nval_data = StanfordDroneDataset(csv='./stanford-drone-dataset-frames/train.csv')","metadata":{"cellId":"y2wx83oirserncl9mc5lni","trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Проверим и визуализурием работу датасета.","metadata":{"cellId":"ldxk4r54ytrqy2ceipqhb"}},{"cell_type":"code","source":"BOX_COLOR = (255, 0, 0) # Red\nLINE_COLOR = (100, 100, 255)\nTEXT_COLOR = (255, 255, 255) # White\n\n\ndef visualize_bbox(img, bbox, label, color=BOX_COLOR, thickness=2):\n    \"\"\"Visualizes a single bounding box on the image\"\"\"\n    x_min, y_min, x_max, y_max = [int(_) for _ in bbox]\n\n    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n\n    ((text_width, text_height), _) = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n    cv2.putText(\n        img,\n        text=label,\n        org=(x_min, y_min - int(0.3 * text_height)),\n        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n        fontScale=0.35, \n        color=TEXT_COLOR, \n        lineType=cv2.LINE_AA,\n    )\n    return img\n\n\ndef visualize(image, bboxes, category_ids, category_id_to_name, grid=None, centers=False, target_obj_mask=None):\n    img = image.copy()\n    if grid is not None:\n        height, width, _ = img.shape\n        nh, nw = grid\n        \n        for i in range(1, nw):\n            x = (width // nw) * i\n            cv2.line(img, (x, 0), (x, height), LINE_COLOR, 1)\n        \n        for j in range(1, nh):\n            y = (height // nh) * j\n            cv2.line(img, (0, y), (width, y), LINE_COLOR, 1)\n\n    for bbox, category_id in zip(bboxes, category_ids):\n        class_name = category_id_to_name[category_id]\n        img = visualize_bbox(img, bbox, class_name)\n    plt.figure(figsize=(12, 12))\n    plt.axis('off')\n    plt.imshow(img)","metadata":{"cellId":"3x56loln1zyrvoax2gqxl","trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def show_examples(dataset, num_examples: int = 4):\n    for i in range(num_examples):\n        k = np.random.randint(0, 80000)\n        target = dataset[k]\n        visualize(target['image'], target['bboxes'], target['labels'], CLASSES, grid=(7, 7))\n    \nshow_examples(dataset=train_data)","metadata":{"cellId":"x9mfqwbtxi88ddptsb9b54","trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Задание 2\n(0.2 балла) сделайте подготовку данных на Albumentations, collate_fn и правильный даталоадер, проверьте шейпы выходных тензоров","metadata":{"cellId":"zeuloiafmyec7yp9njxmlm"}},{"cell_type":"markdown","source":"#### Albumentations\n\nНа ShiftScaleRotate ловлю трейсбек, воспользуемся HorizontalFlip","metadata":{"cellId":"52dgqf6utv3g4f3j5zgrbi"}},{"cell_type":"code","source":"target = train_data[2]","metadata":{"cellId":"6m63tpgbwu4iadb7omfkwg","trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"np.random.seed(2)\ntransform = A.Compose(\n    [ \n        A.HorizontalFlip(p=0.5), \n        A.PadIfNeeded(min_height=512, min_width=512),\n        A.RandomCrop(512, 512, always_apply=True),\n        A.Normalize(),\n        APT.ToTensorV2()\n    ],\n    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'],  min_visibility=.5),\n)\n\ntransformed = transform(**target)","metadata":{"cellId":"u3orjmorzvay3kudqel","trusted":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"Проверка преобразований, но с тензорами не работает","metadata":{"cellId":"u8z457j25eh5qyeps3kjs"}},{"cell_type":"code","source":"# visualize(transformed['image'], transformed['bboxes'], transformed['labels'], CLASSES, grid=(7, 7))","metadata":{"cellId":"41z91lm9hdc9tv3w71pj7t","trusted":true},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"#### collate_fn","metadata":{"cellId":"o7otxihrgihpl02flf9pn"}},{"cell_type":"code","source":"def collate_fn(lst):\n    tmp = defaultdict(list)\n    ret = dict()\n    for entry in lst:\n        if entry['labels'] == []:\n            continue\n        for k, v in entry.items():\n            tmp[k].append(v)\n    \n    for k, v in tmp.items():\n        if isinstance(v[0], np.ndarray):\n            v = np.concatenate(v, 0)\n        if isinstance(v[0], torch.Tensor):\n            v = torch.cat(v, 0)\n        ret[k] = v\n    return ret","metadata":{"cellId":"88mtlz1o2yi5ejbdp4cfa9","trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"trainset = StanfordDroneDataset(csv='./stanford-drone-dataset-frames/train.csv', transform=transform)  \nvalset = StanfordDroneDataset(csv='./stanford-drone-dataset-frames/val.csv', transform=transform)  \n\ndl = DataLoader(trainset, shuffle=True, batch_size=32, collate_fn=collate_fn)","metadata":{"cellId":"63ir79q6a2p3xsva53o2df","trusted":true},"outputs":[],"execution_count":36},{"cell_type":"code","source":"batch = next(iter(dl))","metadata":{"cellId":"8r4fasbvh5456px2k8brl","trusted":true},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"# --- пока так дальше доделаю, когда разберусь с Лоссом ","metadata":{"cellId":"kxi21dl7a3eejups9hd508"}},{"cell_type":"markdown","source":"## Задание 3\n(0.4 балла) Приготовьте модель для детекции, проверьте, что все правильно отрабатывает.","metadata":{"cellId":"6d89bwf0tewp6ge4g9rx4"}},{"cell_type":"code","source":"class VeryModel(nn.Module):\n    def __init__(self):\n        pass\n    \n    def forward(self, x):\n        pass\n    \n    def compute_all(self, batch):\n        pass\n\nnet = VeryModel()\nnet.compute_all(batch)","metadata":{"cellId":"pqkam7djubqfs86yxu42"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Задание 4\n(0.2 балла) натренируйте модель:\n- Убедитесь, что она учится,\n- Проверьте, что на выходе что-то адекватное.\n\nТрейнер можно взять с любого занятия.\n","metadata":{"cellId":"pdj5kimripp1dohjw1jg"}},{"cell_type":"code","source":"","metadata":{"cellId":"gigqqccrawklm3ujmzw6d"},"outputs":[],"execution_count":null}]}